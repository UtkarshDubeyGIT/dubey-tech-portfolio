---
title: "Building a Machine Learning Web App: From Jupyter Notebook to Production"
publishedAt: "15-03-2025"
summary: "The complete journey of turning a regression model into an interactive web application. Learn about feature engineering, model selection, deployment strategies, and making ML accessible to non-technical users."
image: "/images/projects/project-01/laptop-predictor-1.png"
tag: "Machine Learning"
---

## The Problem:  Laptop Shopping is Confusing

Ever tried buying a laptop? You're bombarded with specs:  i5 vs i7, 8GB vs 16GB RAM, SSD vs HDD, IPS vs TN panels. **What does it all mean for the price?**

This project started with a simple question: **Can a machine learning model predict laptop prices from specifications?** Turns out, yes‚Äîand the journey from Jupyter notebook to deployed web app taught me more about ML engineering than months of theory.

## The Dataset: 1,300 Laptops, 12 Features

### Data Collection

```python
# laptop_data.csv structure
columns = [
    'Company', 'TypeName', 'Inches', 'ScreenResolution',
    'Cpu', 'Ram', 'Memory', 'Gpu', 'OpSys', 'Weight',
    'Price' # Target variable
]
```

**Source**: Scraped from e-commerce sites (Flipkart, Amazon India)  
**Price Range**: ‚Çπ10,000 ‚Äì ‚Çπ3,00,000  
**Brands**: Dell, HP, Lenovo, Apple, Asus, Acer, MSI, etc.

### Initial Observations

- **Highly imbalanced brands**: Dell (22%), HP (18%), Apple (7%)
- **Categorical hell**: CPU names like "Intel Core i7 8550U 1.8GHz" need parsing
- **Nested features**: Screen resolution contains both dimensions AND panel type (IPS/TN)

## Feature Engineering: The Make-or-Break Stage

Raw data is useless. The model learns from **how you present the data**.

### 1. Extracting CPU Information

```python
def extract_cpu_features(cpu_string):
    """
    Input: "Intel Core i7 8550U 1.8GHz"
    Output: {
        'brand': 'Intel',
        'tier': 'i7',
        'generation': 8,
        'clockspeed':  1.8
    }
    """
    patterns = {
        'intel_i':  r'Intel Core (i\d)',
        'amd_ryzen': r'AMD Ryzen (\d)',
        'generation': r'(\d{4}[A-Z])',
        'speed': r'(\d\.\d+)GHz'
    }
    # ...  regex extraction logic
```

**Why this matters**: "i7" is more predictive than "Intel." Generation matters (11th gen > 8th gen).

### 2. Deconstructing Screen Resolution

```python
def parse_screen(resolution_string):
    """
    Input: "Full HD 1920x1080 IPS Panel"
    Output: {
        'ppi': 141. 2,  # Pixels per inch
        'touchscreen': False,
        'ips': True
    }
    """
    width, height = extract_dimensions(resolution_string)
    ppi = calculate_ppi(width, height, screen_inches)
    
    return {
        'ppi': ppi,
        'touchscreen':  'Touchscreen' in resolution_string,
        'ips': 'IPS' in resolution_string
    }
```

**Key insight**: PPI (pixels per inch) is more meaningful than raw resolution.  A 4K display on 13" is sharper than on 17".

### 3. Memory Type Classification

```python
def parse_memory(memory_string):
    """
    Input: "512GB SSD + 1TB HDD"
    Output:  {
        'ssd_gb': 512,
        'hdd_gb': 1000,
        'memory_type': 'hybrid'
    }
    """
    ssd = extract_storage(memory_string, storage_type='SSD')
    hdd = extract_storage(memory_string, storage_type='HDD')
    
    return {
        'ssd_gb': ssd,
        'hdd_gb':  hdd,
        'memory_type': 'ssd' if ssd else 'hdd' if hdd else 'hybrid'
    }
```

**Rationale**: SSDs drastically affect price.  A 256GB SSD laptop costs more than 1TB HDD.

### 4. GPU Tier Extraction

```python
gpu_tiers = {
    'integrated':  ['Intel HD', 'Intel UHD', 'AMD Radeon'],
    'entry': ['Nvidia MX', 'GTX 1050'],
    'mid': ['GTX 1650', 'GTX 1660', 'RTX 3050'],
    'high': ['RTX 3060', 'RTX 3070', 'RTX 4060']
}
```

Gaming laptops with RTX 3070 are priced 2-3x higher than integrated graphics models.

## Model Selection: The Beauty Contest

Tried multiple regression algorithms.  Here's the showdown:

| Model                 | R¬≤ Score | RMSE (‚Çπ) | Training Time |
| --------------------- | -------- | -------- | ------------- |
| Linear Regression     | 0.68     | 24,500   | < 1s          |
| Decision Tree         | 0.79     | 18,200   | < 1s          |
| Random Forest         | **0.86** | 14,100   | 8s            |
| Gradient Boosting     | **0.88** | 13,200   | 15s           |
| XGBoost               | 0.87     | 13,500   | 12s           |

**Winner**: **Random Forest** (best trade-off between accuracy and inference speed for web deployment).

### Why Random Forest?

```python
from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=15,
    min_samples_split=5,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)
```

**Advantages**: 

- Handles non-linear relationships (price vs RAM isn't linear)
- Robust to outliers (‚Çπ2L Apple laptops don't break the model)
- Feature importance for free (see below)

### Feature Importance Insights

```python
top_features = rf_model.feature_importances_. argsort()[-10:]

# Top 10 most predictive features: 
1. RAM (GB)          - 18.2%
2. SSD Capacity      - 15.7%
3. CPU Tier (i3/i5/i7) - 12.4%
4. GPU Tier          - 11.8%
5. Brand (Apple)     - 9.3%
6. Screen PPI        - 7.6%
7. Weight (kg)       - 6.1%
8. IPS Panel         - 5.2%
9. Touchscreen       - 4.9%
10. CPU Generation   - 4.1%
```

**Surprise finding**: Brand matters more than CPU generation for price!  Apple tax is real.

## Building the Streamlit Web App

MLmodels in notebooks are useless to non-technical users. Enter **Streamlit**‚Äîturn Python scripts into interactive web apps.

### App Architecture

```
app. py                    # Main Streamlit app
‚îú‚îÄ‚îÄ load_model()          # Unpickle trained pipeline
‚îú‚îÄ‚îÄ sidebar_inputs()      # User selection widgets
‚îú‚îÄ‚îÄ predict_price()       # Model inference
‚îú‚îÄ‚îÄ show_similar()        # Find comparable laptops
‚îî‚îÄ‚îÄ visualize_data()      # EDA charts
```

### The Magic of Streamlit

```python
import streamlit as st
import pickle

# Load pre-trained model
@st.cache_resource
def load_model():
    with open('pipe.pkl', 'rb') as f:
        return pickle.load(f)

model = load_model()

# User inputs
brand = st.selectbox('Brand', ['Dell', 'HP', 'Lenovo', 'Apple'])
ram = st.slider('RAM (GB)', 4, 64, 8, step=4)
cpu = st.selectbox('Processor', ['Core i3', 'Core i5', 'Core i7', 'Ryzen 5'])
gpu = st.selectbox('GPU', ['Integrated', 'MX', 'GTX 1650', 'RTX 3060'])

if st.button('Predict Price'):
    # Feature engineering pipeline
    features = create_feature_vector(brand, ram, cpu, gpu, ...)
    
    # Inference
    price = model.predict([features])[0]
    
    st.success(f'üí∞ Estimated Price: ‚Çπ{price: ,.0f}')
```

### Interactive Features

#### 1. Price Prediction

Users select specs ‚Üí Model predicts price in real-time. 

#### 2. Similar Laptop Finder

```python
def find_similar(predicted_price, tolerance=5000):
    """Show real laptops within ‚Çπ5k of prediction"""
    similar = df[
        (df['Price'] >= predicted_price - tolerance) &
        (df['Price'] <= predicted_price + tolerance)
    ]
    return similar[['Company', 'TypeName', 'Ram', 'Price']].head(5)
```

**Why useful**: Users see *actual laptops* matching their budget.

#### 3. Exploratory Data Analysis

```python
tab1, tab2, tab3 = st.tabs(['Price Distribution', 'Brand Comparison', 'Feature Impact'])

with tab1:
    fig = px.histogram(df, x='Price', nbins=50, title='Laptop Price Distribution')
    st.plotly_chart(fig)

with tab2:
    avg_price = df. groupby('Company')['Price'].mean().sort_values()
    st.bar_chart(avg_price)

with tab3:
    st.write('**Feature Importance**')
    st.bar_chart(feature_importances)
```

**Impact**: Users explore data before trusting predictions.

## Deployment: From Localhost to Cloud

### Local Development

```bash
pip install -r requirements.txt
streamlit run app.py
# Opens http://localhost:8501
```

### Cloud Deployment (Render. com)

**Why Render?** Free tier, automatic HTTPS, GitHub integration. 

#### 1. Create `requirements.txt`

```txt
streamlit==1.28.0
pandas==2.1.0
scikit-learn==1.3.0
plotly==5.17.0
```

#### 2. Add `Procfile`

```
web: sh setup.sh && streamlit run app.py
```

#### 3. Configure `setup.sh`

```bash
mkdir -p ~/.streamlit/
echo "\
[server]\n\
headless = true\n\
port = $PORT\n\
enableCORS = false\n\
\n\
" > ~/.streamlit/config.toml
```

#### 4. Deploy

```bash
git push origin main
# Render auto-detects Procfile and deploys
```

**Live URL**: `https://laptop-price-predictor-xyz.onrender.com`

## Challenges That Taught Me Lessons

### 1. Pickle Versioning Hell

Trained model on scikit-learn 1.2, deployed app with 1.3 ‚Üí **Model failed to load.**

**Solution**: Pin exact versions in `requirements.txt`:

```txt
scikit-learn==1.2.2  # Match training environment
```

### 2. Categorical Encoding Breaks on New Data

Trained with 25 brands.  User selects "Samsung" (not in training data) ‚Üí **Error.**

**Solution**: Use `handle_unknown='ignore'` in OneHotEncoder:

```python
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
```

### 3. Streamlit Memory Leaks

App crashed after 50 predictions.  Culprit: **Loading model on every prediction.**

**Solution**: Cache model with `@st.cache_resource`:

```python
@st.cache_resource
def load_model():
    return pickle.load(open('pipe. pkl', 'rb'))
```

Memory usage: 500MB ‚Üí 120MB. 

## Model Performance Deep Dive

### Regression Metrics

```python
from sklearn.metrics import mean_absolute_error, r2_score

y_pred = model.predict(X_test)

print(f'R¬≤ Score: {r2_score(y_test, y_pred):.3f}')
# 0.862

print(f'MAE: ‚Çπ{mean_absolute_error(y_test, y_pred):,.0f}')
# ‚Çπ12,400

print(f'MAPE: {mean_absolute_percentage_error(y_test, y_pred):.2%}')
# 18.5%
```

**Translation**: Model explains 86% of price variance.  Average error: ‚Çπ12k (18%).

### Where the Model Struggles

```python
# Find worst predictions
errors = abs(y_test - y_pred)
worst = df. iloc[errors.nlargest(5).index]

# Worst prediction:  ‚Çπ45k off
# Actual: ‚Çπ2,10,000 | Predicted: ‚Çπ1,65,000
# Laptop:  MacBook Pro 16" (M1 Max)
```

**Why? ** Apple's premium pricing defies specs-based logic.  The model doesn't account for brand premium.

## Key Takeaways for ML Beginners

1. **Feature engineering > Model choice**:  Better features with Linear Regression beat raw data with XGBoost
2. **Domain knowledge is crucial**: Knowing "PPI matters more than resolution" comes from understanding screens
3. **Streamlit makes ML accessible**: No need for React/Vue‚Äîturn notebooks into apps in 50 lines
4. **Deployment is 50% of the project**: Working model ‚â† useful product
5. **Always visualize feature importance**: Users trust models they understand

## What I'd Improve Next

1. **Scrape fresh data**: Current dataset is 2022.  Prices and models change rapidly.
2. **Add time-series analysis**: Track price trends (e.g., "i7 laptops dropped 15% in 6 months").
3. **Implement sentiment analysis**: Scrape reviews to factor in quality/reliability.
4. **A/B test feature sets**: Does removing "weight" improve accuracy? 

## Tech Stack Breakdown

| Component         | Technology      | Purpose                      |
| ----------------- | --------------- | ---------------------------- |
| Data Processing   | pandas, numpy   | Feature engineering          |
| Modeling          | scikit-learn    | Random Forest, pipelines     |
| Visualization     | matplotlib, seaborn, plotly | EDA, insights |
| Web Framework     | Streamlit       | Interactive UI               |
| Deployment        | Render. com      | Cloud hosting, CI/CD         |
| Model Persistence | pickle          | Save/load trained pipeline   |

## The Real Learning

This project taught me that **ML isn't just about algorithms‚Äîit's about the entire pipeline**:

1. **Data collection**: Garbage in, garbage out
2. **Feature engineering**: Where 80% of insights happen
3. **Model training**: The "easy" part
4. **Evaluation**: Metrics that actually matter
5. **Deployment**:  Making it usable by humans
6. **Iteration**: Models aren't static‚Äîretrain, improve, repeat

**Machine learning engineering is 20% science, 80% software engineering.**

---

**Live Demo**: [Laptop Price Predictor](https://laptop-price-predictor.onrender.com)  
**Repository**: [Laptop-Price-Predictor](https://github.com/UtkarshDubeyGIT/Laptop-Price-Predictor)  
**Tech Stack**: Python, scikit-learn, Streamlit, pandas, Render.com

*Built with ‚ù§Ô∏è for making ML accessible to everyone*